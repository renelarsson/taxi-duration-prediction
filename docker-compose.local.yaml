version: "3.8"

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.local
    image: ${LOCAL_IMAGE_NAME}
    ports:
      - "8080:8080"
    environment:
      AWS_REGION: ${AWS_REGION}
      MLFLOW_TRACKING_URI: http://mlflow:5000
      PREDICTIONS_STREAM_NAME: ${PREDICTIONS_STREAM_NAME}
      RUN_ID: ${RUN_ID}
      AWS_DEFAULT_REGION: ${AWS_REGION}
      MODEL_LOCATION: /var/task/model
      # LocalStack endpoints for local development
      KINESIS_ENDPOINT_URL: http://localstack:4566
      MLFLOW_S3_ENDPOINT_URL: http://localstack:4566
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
    depends_on:
      - localstack
      - mlflow
    networks:
      - taxi-duration-prediction

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.5.0
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow-artifacts/
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - MLFLOW_S3_ENDPOINT_URL=http://localstack:4566
    volumes:
      - mlflow-data:/mlflow
    depends_on:
      - localstack
    networks:
      - taxi-duration-prediction
    command: mlflow server --host 0.0.0.0 --port 5000

  localstack:
    image: localstack/localstack:2.2
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3,kinesis
      - DEBUG=1
      - DATA_DIR=/tmp/localstack/data
      - DOCKER_HOST=unix:///var/run/docker.sock
      - HOST_TMP_FOLDER=/tmp/localstack
    volumes:
      - ./localstack-data:/tmp/localstack/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - taxi-duration-prediction

volumes:
  mlflow-data:

networks:
  taxi-duration-prediction:
    driver: bridge